{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_JPEG.zip\n",
    "!mv ISIC_2020_Training_JPEG.zip data/jpeg.zip\n",
    "!unzip data/jpeg.zip -d data/jpeg\n",
    "!rename.ul jpg jpeg data/jpeg/train/*.jpg\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_GroundTruth.csv\n",
    "!mv ISIC_2020_Training_GroundTruth.csv data/train.csv\n",
    "!rm data/jpeg.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7a24a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: '/Users/andresarpi/anaconda3/envs/tf2/lib/python3.8/site-packages/pybind11-2.7.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "#import keras_tuner as kt\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c268727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'jpeg', 'train')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TF_PREFIX = 'train'\n",
    "TRAIN_SPLIT = 0.9\n",
    "SMALL_SPLIT = 0.01\n",
    "IMAGE_SIZE = (224, 222)\n",
    "BATCH_SIZE = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffb696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 09:14:57.415976: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "age_normalization_fn = lambda x :  x / 90 if np.isfinite(x) else 0.5\n",
    "sex_nomalization_fn = lambda x : 1.0 if x == \"male\" else 0.0\n",
    "\n",
    "class CsvTransformer:\n",
    "    cols = ['age', 'sex', 'target']\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['age'] = df['age_approx'].apply(age_normalization_fn).values\n",
    "        df['sex'] = df['sex'].apply(sex_nomalization_fn).values\n",
    "        df['target'] = df['target'].astype(\"float64\")\n",
    "        df = df.set_index('image_name')\n",
    "        self.df = df\n",
    "        self.init_tables()\n",
    "        \n",
    "    def init_tables(self):\n",
    "        # build a lookup table\n",
    "        self.lookup_tables = {}\n",
    "        for col in self.cols:\n",
    "            table = tf.lookup.StaticHashTable(\n",
    "                initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                    keys=tf.constant(list(self.df.index.values)),\n",
    "                    values=tf.constant(list(self.df[[col]].values.flatten())),\n",
    "                ),\n",
    "                default_value=tf.constant(-1.0, dtype=tf.float64),\n",
    "                name=\"class_weight\"\n",
    "            )\n",
    "            \n",
    "            self.lookup_tables[col] = table\n",
    "        \n",
    "    def get_vector_from_image_name(self, col, image_name):\n",
    "            \n",
    "        return self.lookup_tables[col].lookup(image_name)\n",
    "    \n",
    "transformer = CsvTransformer(TRAIN_CSV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bda2cd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28013, 7), (3113, 7), (311, 7), (2000, 7))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "original_df = pd.read_csv(TRAIN_CSV).sample(frac=1).reset_index(drop=True)\n",
    "original_df.set_index('image_name', inplace = True)\n",
    "\n",
    "validation_df = original_df.iloc[:2000]\n",
    "original_df = original_df.iloc[2000:]\n",
    "\n",
    "split_point = int(len(original_df) * TRAIN_SPLIT)\n",
    "small_split_point = int(len(original_df) * SMALL_SPLIT)\n",
    "\n",
    "train_df = original_df.iloc[:split_point]\n",
    "test_df = original_df[split_point:]\n",
    "small_df = original_df[:small_split_point]\n",
    "\n",
    "train_df.shape, test_df.shape, small_df.shape, validation_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a25cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_fn(filename):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "\n",
    "    image_name = tf.strings.split(filename, sep='/')[-1]\n",
    "    image_name = tf.strings.split(image_name, sep='.')[0]\n",
    "    sex = transformer.get_vector_from_image_name('sex', image_name)\n",
    "    age = transformer.get_vector_from_image_name('age', image_name)\n",
    "    target = tf.cast(transformer.get_vector_from_image_name('target', image_name), dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image, \n",
    "            \"image_name\": image_name,\n",
    "            \"sex\": sex,\n",
    "            \"age\": age}, target\n",
    "\n",
    "def get_dataset(df: pd.DataFrame, batch_size: int):\n",
    "    filenames = IMAGES_DIR + '/' + df.index.values + \".jpeg\"\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    ds = ds.map(map_fn)\\\n",
    "            .cache() \\\n",
    "            .prefetch(tf.data.AUTOTUNE)\\\n",
    "            .batch(batch_size)\n",
    "\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a370de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = get_dataset(train_df, BATCH_SIZE)\n",
    "test_dataset = get_dataset(test_df, BATCH_SIZE)\n",
    "small_dataset = get_dataset(small_df, BATCH_SIZE)\n",
    "validation_df = get_dataset(validation_df, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d44061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Examples:\n",
      "    Total: 29813\n",
      "    Positive: 526 (1.76% of total)\n",
      "\n",
      "test Examples:\n",
      "    Total: 3313\n",
      "    Positive: 58 (1.75% of total)\n",
      "\n",
      "small Examples:\n",
      "    Total: 331\n",
      "    Positive: 5 (1.51% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [(train_df, \"Train\"), (test_df, \"test\"), (small_df, \"small\")]\n",
    "\n",
    "for df, df_name in dfs:\n",
    "\n",
    "    neg, pos = np.bincount(df['target'])\n",
    "    total = neg + pos\n",
    "    print('{} Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        df_name, total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6493c17",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c75d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,  preprocessor = tf.keras.applications.mobilenet_v3.preprocess_input,\n",
    "                        network = tf.keras.applications.MobileNetV3Large,\n",
    "                        pooling = \"max\",\n",
    "                        dense_intermediate = -1,\n",
    "                        dropout = 0.5,\n",
    "                        extra_cols = [\"sex\", \"age\"],\n",
    "                        extra_cols_out = 32,\n",
    "                        bias = None):\n",
    "                        \n",
    "        super().__init__()\n",
    "        self.extra_cols = extra_cols\n",
    "        self.image_preprocessor = preprocessor\n",
    "        self.image_feature_extractor = network(weights='imagenet', include_top = False, pooling = pooling)\n",
    "\n",
    "        self.image_size = IMAGE_SIZE\n",
    "\n",
    "        self.extra_cols_dense = tf.keras.layers.Dense(extra_cols_out, activation = \"relu\", name=\"extra_cols_dense\")\n",
    "        self.extra_cols_flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.use_intermediate = dense_intermediate > 0\n",
    "        if self.use_intermediate:\n",
    "            self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "            self.dense1 = tf.keras.layers.Dense(dense_intermediate, activation=\"relu\", name = \"final_dense_intermediate\")\n",
    "\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "        self.out = tf.keras.layers.Dense(1, activation = \"sigmoid\", bias_initializer=bias, name = \"final_dense\")\n",
    "\n",
    "    def preprocess_images(self, images):\n",
    "        x = self.image_preprocessor(images)\n",
    "        x = tf.image.resize(x, self.image_size)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs):\n",
    "        preprocessed_images = self.preprocess_images(inputs['image'])\n",
    "        image_output = self.image_feature_extractor(preprocessed_images, training = False)\n",
    "\n",
    "        reshaped = []\n",
    "        for col in self.extra_cols:\n",
    "            reshaped.append(inputs[col])\n",
    "        reshaped = tf.stack(reshaped, axis = -1)\n",
    "\n",
    "        reshaped = self.extra_cols_dense(reshaped)\n",
    "        reshaped = self.extra_cols_flatten(reshaped)\n",
    "\n",
    "        x = tf.concat([reshaped, image_output], axis = -1)\n",
    "        if self.use_intermediate:\n",
    "            x = self.dropout1(x)\n",
    "            x = self.dense1(x)\n",
    "            \n",
    "        x = self.dropout2(x)\n",
    "        out = self.out(x)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ee9c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPU found\n",
      "Devices: [] 0\n",
      "No devices found, using default\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "except ValueError:\n",
    "    print(\"No TPU found\")\n",
    "    devices_found = tf.config.list_physical_devices('GPU')\n",
    "    print(\"Devices:\", devices_found, len(devices_found))\n",
    "    if len(devices_found) < 1:\n",
    "        print(\"No devices found, using default\")\n",
    "        strategy = tf.distribute.get_strategy() \n",
    "    else:\n",
    "        print(\"Devices found, using mirrored\")\n",
    "        strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca483f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    output_bias = tf.keras.initializers.Constant(np.log([500 / 30000]))\n",
    "    model = MyModel(bias=output_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0da8fb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "MobilenetV3large (Functional (None, 1280)              4226432   \n",
      "_________________________________________________________________\n",
      "extra_cols_dense (Dense)     multiple                  96        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "final_dense (Dense)          multiple                  1313      \n",
      "=================================================================\n",
      "Total params: 4,227,841\n",
      "Trainable params: 4,203,441\n",
      "Non-trainable params: 24,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for el, target in small_dataset:\n",
    "    model(el)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99b0ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results before training\n",
      "loss :  6.3953166007995605\n",
      "tp :  5.0\n",
      "fp :  324.0\n",
      "tn :  2.0\n",
      "fn :  0.0\n",
      "accuracy :  0.021148037165403366\n",
      "precision :  0.015197568573057652\n",
      "recall :  1.0\n",
      "auc :  0.5352761149406433\n",
      "prc :  0.01687612757086754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = small_dataset\n",
    "\n",
    "METRICS = [\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=METRICS,\n",
    "              )\n",
    "\n",
    "baseline_results = model.evaluate(ds,\n",
    "                                  batch_size=BATCH_SIZE, \n",
    "                                  verbose=0)\n",
    "                                  \n",
    "print(\"Train results before training\")                       \n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbb1bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 55s 14s/step - loss: 0.0719 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 553.3333 - fn: 8.6667 - accuracy: 0.9846 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8068 - prc: 0.0406 - val_loss: 0.0702 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9264 - val_prc: 0.0928\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 45s 13s/step - loss: 0.0718 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8094 - prc: 0.0597 - val_loss: 0.0676 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9206 - val_prc: 0.0842\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 44s 15s/step - loss: 0.0635 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9301 - prc: 0.4671 - val_loss: 0.0632 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9472 - val_prc: 0.1413\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 51s 13s/step - loss: 0.0632 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9004 - prc: 0.1398 - val_loss: 0.0610 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9567 - val_prc: 0.1522\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 48s 13s/step - loss: 0.0596 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9477 - prc: 0.1256 - val_loss: 0.0591 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9626 - val_prc: 0.1892\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 49s 15s/step - loss: 0.0667 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8865 - prc: 0.0616 - val_loss: 0.0537 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9853 - val_prc: 0.3895\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 49s 14s/step - loss: 0.0557 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9589 - prc: 0.1927 - val_loss: 0.0526 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9850 - val_prc: 0.5347\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 52s 14s/step - loss: 0.0638 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8764 - prc: 0.0901 - val_loss: 0.0507 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9871 - val_prc: 0.6144\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 50s 15s/step - loss: 0.0547 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9419 - prc: 0.2101 - val_loss: 0.0498 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9880 - val_prc: 0.6480\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 51s 16s/step - loss: 0.0493 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 301.3333 - fn: 4.6667 - accuracy: 0.9847 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9848 - prc: 0.4652 - val_loss: 0.0463 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 326.0000 - val_fn: 5.0000 - val_accuracy: 0.9849 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9837 - val_prc: 0.5690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa52ad88940>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 3e-5\n",
    "epochs = 10\n",
    "train_ds = small_dataset\n",
    "test_ds = small_dataset\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = lr),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            metrics=METRICS,\n",
    "            )\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(  train_ds, \n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stopping],\n",
    "            validation_data=test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "\n",
    "    hp_extra_cols = hp.Choice('extra_cols', values=[[\"sex\", \"age\"], ['sex'], ['age']])\n",
    "    hp_extra_cols_out = hp.Choice('extra_cols_out', values=[1, 32, 128])\n",
    "    hp_intermediate = hp.Choice('intermediate', values=[-1, 32, 128])\n",
    "\n",
    "    MyModel(extra_cols=hp_extra_cols, extra_cols_out=hp_extra_cols_out, dense_intermediate=hp_intermediate)\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 3e-5, 1e-5])\n",
    "\n",
    "    model.compile(  optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_ds, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "243e99ab9e43edd82599dfc00766f0e8204baa42021f8d2790e6b99e9ee445ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7738771",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_JPEG.zip\n",
    "!mv ISIC_2020_Training_JPEG.zip data/jpeg.zip\n",
    "!unzip data/jpeg.zip -d data/jpeg\n",
    "!rename.ul jpg jpeg data/jpeg/train/*.jpg\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_GroundTruth.csv\n",
    "!mv ISIC_2020_Training_GroundTruth.csv data/train.csv\n",
    "!rm data/jpeg.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d2db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mel/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb3e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mel/bin/jupyter\r\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018d15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import MyModel\n",
    "from data_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b90e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'jpeg', 'train')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TF_PREFIX = 'train'\n",
    "TRAIN_SPLIT = 0.9\n",
    "SMALL_SPLIT = 0.01\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1230c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28013, 7), (3113, 7), (311, 7), (2000, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "original_df = pd.read_csv(TRAIN_CSV).sample(frac=1).reset_index(drop=True)\n",
    "original_df.set_index('image_name', inplace = True)\n",
    "\n",
    "validation_df = original_df.iloc[:2000]\n",
    "original_df = original_df.iloc[2000:]\n",
    "\n",
    "split_point = int(len(original_df) * TRAIN_SPLIT)\n",
    "small_split_point = int(len(original_df) * SMALL_SPLIT)\n",
    "\n",
    "train_df = original_df.iloc[:split_point]\n",
    "test_df = original_df[split_point:]\n",
    "small_df = original_df[:small_split_point]\n",
    "\n",
    "train_df.shape, test_df.shape, small_df.shape, validation_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a27c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CsvTransformer(TRAIN_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc307f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(filename):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "\n",
    "    image_name = tf.strings.split(filename, sep='/')[-1]\n",
    "    image_name = tf.strings.split(image_name, sep='.')[0]\n",
    "    data = transformer.get_data_vector(image_name)\n",
    "    target = tf.cast(transformer.get_vector_from_image_name('target', image_name), dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image, \n",
    "            \"image_name\": image_name,\n",
    "            \"data\": data}, target\n",
    "\n",
    "def get_dataset(df: pd.DataFrame, images_dir, batch_size: int, cache = True):\n",
    "    filenames = images_dir + '/' + df.index.values + \".jpeg\"\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    ds = ds.map(map_fn)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e9b4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = get_dataset(train_df, BATCH_SIZE, cache = True)\n",
    "small_train_dataset = get_dataset(train_df.iloc[:2000], IMAGES_DIR, BATCH_SIZE)\n",
    "test_dataset = get_dataset(test_df, IMAGES_DIR, BATCH_SIZE)\n",
    "small_dataset = get_dataset(small_df, IMAGES_DIR, BATCH_SIZE)\n",
    "validation_dataset = get_dataset(validation_df, IMAGES_DIR, BATCH_SIZE, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a57dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Examples:\n",
      "    Total: 3113\n",
      "    Positive: 52 (1.67% of total)\n",
      "\n",
      "small Examples:\n",
      "    Total: 311\n",
      "    Positive: 3 (0.96% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [(test_df, \"test\"), (small_df, \"small\")]\n",
    "\n",
    "for df, df_name in dfs:\n",
    "\n",
    "    neg, pos = np.bincount(df['target'])\n",
    "    total = neg + pos\n",
    "    print('{} Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        df_name, total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1907d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top.h5\n",
      "17612800/17605208 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<model.MyModel at 0x7ffb14aefb10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_model = MyModel.create_standard_version(load_weights_path=\"weights/\", compile=True)\n",
    "weights_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ca6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 367s 4s/step - loss: 0.0868 - tp: 0.0000e+00 - fp: 0.7576 - tn: 1568.7071 - fn: 29.7475 - accuracy: 0.9774 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8540 - prc: 0.1658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06712061166763306,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 3060.0,\n",
       " 52.0,\n",
       " 0.982974648475647,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8586528301239014,\n",
       " 0.14941127598285675]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95051faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 238s 4s/step - loss: 0.0712 - tp: 1.0000 - fp: 1.0000 - tn: 1963.0000 - fn: 35.0000 - accuracy: 0.9820 - precision: 0.5000 - recall: 0.0278 - auc: 0.8723 - prc: 0.1426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07123943418264389,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1963.0,\n",
       " 35.0,\n",
       " 0.9819999933242798,\n",
       " 0.5,\n",
       " 0.02777777798473835,\n",
       " 0.8723480701446533,\n",
       " 0.14258483052253723]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfabdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stats(dataset, threshold):\n",
    "    predictions, targets = [], []\n",
    "    for el, target in dataset:\n",
    "        preds = weights_model.predict(el)\n",
    "        predictions.append(preds)\n",
    "        targets.append(target.numpy())\n",
    "    \n",
    "    predictions, targets = np.vstack(predictions), np.expand_dims(np.hstack(targets), 1)\n",
    "    print(f\"pred stats - mean: {predictions.mean()}, std: {predictions.std()}\")\n",
    "    deciles = np.percentile(predictions, np.arange(10, 100, 10))\n",
    "    print(f\"pred deciles: {deciles}\")\n",
    "    thresh_predictions = (predictions > threshold).astype(\"int\")\n",
    "    res = tf.math.confusion_matrix(labels=targets.flatten(), predictions=thresh_predictions.flatten())\n",
    "    true_positives, false_positives, true_negatives, false_negatives = res[1, 1], res[0, 1], res[0, 0], res[1, 0]\n",
    "    print(\"true_positives: %d, false_positives: %d, true_negatives: %d, false_negatives: %d\" % (true_positives, false_positives, true_negatives, false_negatives))\n",
    "    sensitivity = true_positives / (true_positives + false_negatives) * 100.0\n",
    "    specificity = true_negatives /(true_negatives + false_positives) * 100.0\n",
    "\n",
    "    print(f\"sensitivity: {sensitivity:.2f}%, specificity: {specificity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5cfc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred stats - mean: 0.016367629170417786, std: 0.03150975704193115\n",
      "pred deciles: [0.00028777 0.0008963  0.00192593 0.00411637 0.00856157 0.01317607\n",
      " 0.01849228 0.02461386 0.03629587]\n",
      "true_positives: 24, false_positives: 424, true_negatives: 1544, false_negatives: 8\n",
      "sensitivity: 75.00%, specificity: 78.46%\n"
     ]
    }
   ],
   "source": [
    "get_stats(dataset = small_train_dataset, threshold = 0.023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9612d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred stats - mean: 0.017042944207787514, std: 0.031137915328145027\n",
      "pred deciles: [0.0003391  0.00102918 0.00236574 0.00456951 0.00919554 0.01446005\n",
      " 0.01885822 0.02452556 0.03598046]\n",
      "true_positives: 40, false_positives: 649, true_negatives: 2412, false_negatives: 12\n",
      "sensitivity: 76.92%, specificity: 78.80%\n"
     ]
    }
   ],
   "source": [
    "get_stats(dataset = test_dataset, threshold = 0.023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f118037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred stats - mean: 0.017199963331222534, std: 0.032993678003549576\n",
      "pred deciles: [0.00035127 0.00099075 0.00215774 0.00435485 0.00933594 0.01425113\n",
      " 0.01867158 0.0247126  0.0367173 ]\n",
      "true_positives: 29, false_positives: 417, true_negatives: 1547, false_negatives: 7\n",
      "sensitivity: 80.56%, specificity: 78.77%\n"
     ]
    }
   ],
   "source": [
    "get_stats(dataset = validation_dataset, threshold = 0.023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c39a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "243e99ab9e43edd82599dfc00766f0e8204baa42021f8d2790e6b99e9ee445ea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a82422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55880eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'jpeg', 'train')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TF_PREFIX = 'train'\n",
    "TRAIN_SPLIT = 0.9\n",
    "SMALL_SPLIT = 0.01\n",
    "IMAGE_SIZE = (224, 222)\n",
    "BATCH_SIZE = 32\n",
    "MODEL_PATH = \"models/model_v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67579ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 10:15:33.801917: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "age_normalization_fn = lambda x :  x / 90 if np.isfinite(x) else 0.5\n",
    "sex_nomalization_fn = lambda x : 1.0 if x == \"male\" else 0.0\n",
    "\n",
    "class CsvTransformer:\n",
    "    cols = ['sex', 'age', 'anatom_head/neck', 'anatom_lower extremity', 'anatom_oral/genital', 'anatom_palms/soles', \\\n",
    "        'anatom_torso', 'anatom_upper extremity', 'target']\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['age'] = df['age_approx'].apply(age_normalization_fn).values\n",
    "        df['sex'] = df['sex'].apply(sex_nomalization_fn).values\n",
    "        df['target'] = df['target'].astype(\"float64\")\n",
    "        df = pd.get_dummies(df,prefix=['anatom'], columns = ['anatom_site_general_challenge'], drop_first=False)\n",
    "        df = df.set_index('image_name')\n",
    "        self.df = df\n",
    "        self.init_tables()\n",
    "        \n",
    "    def init_tables(self):\n",
    "        # build a lookup table\n",
    "        self.lookup_tables = {}\n",
    "        for col in self.cols:\n",
    "            table = tf.lookup.StaticHashTable(\n",
    "                initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                    keys=tf.constant(list(self.df.index.values)),\n",
    "                    values=tf.constant(list(self.df[[col]].values.astype(\"float64\").flatten())),\n",
    "                ),\n",
    "                default_value=tf.constant(-1.0, dtype=tf.float64),\n",
    "                name=\"class_weight\"\n",
    "            )\n",
    "            \n",
    "            self.lookup_tables[col] = table\n",
    "\n",
    "    def get_data_vector(self, image_names):\n",
    "        vals = []\n",
    "        for col in self.cols:\n",
    "            if col == 'target':\n",
    "                continue\n",
    "            table = self.lookup_tables[col]\n",
    "            val = table.lookup(image_names)\n",
    "            vals.append(val)\n",
    "\n",
    "        return tf.transpose(tf.stack(vals))\n",
    "\n",
    "    def get_vector_from_image_name(self, col, image_name):\n",
    "            \n",
    "        return self.lookup_tables[col].lookup(image_name)\n",
    "    \n",
    "transformer = CsvTransformer(TRAIN_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1f0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28013, 7), (3113, 7), (311, 7), (2000, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "original_df = pd.read_csv(TRAIN_CSV).sample(frac=1).reset_index(drop=True)\n",
    "original_df.set_index('image_name', inplace = True)\n",
    "\n",
    "validation_df = original_df.iloc[:2000]\n",
    "original_df = original_df.iloc[2000:]\n",
    "\n",
    "split_point = int(len(original_df) * TRAIN_SPLIT)\n",
    "small_split_point = int(len(original_df) * SMALL_SPLIT)\n",
    "\n",
    "train_df = original_df.iloc[:split_point]\n",
    "test_df = original_df[split_point:]\n",
    "small_df = original_df[:small_split_point]\n",
    "\n",
    "train_df.shape, test_df.shape, small_df.shape, validation_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_fn(filename):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "\n",
    "    image_name = tf.strings.split(filename, sep='/')[-1]\n",
    "    image_name = tf.strings.split(image_name, sep='.')[0]\n",
    "    data = transformer.get_data_vector(image_name)\n",
    "    target = tf.cast(transformer.get_vector_from_image_name('target', image_name), dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image, \n",
    "            \"image_name\": image_name,\n",
    "            \"data\": data}, target\n",
    "\n",
    "def get_dataset(df: pd.DataFrame, batch_size: int, cache = True):\n",
    "    filenames = IMAGES_DIR + '/' + df.index.values + \".jpeg\"\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    ds = ds.map(map_fn)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c959e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = get_dataset(train_df, BATCH_SIZE, cache = True)\n",
    "small_train_dataset = get_dataset(train_df.iloc[:2000], BATCH_SIZE)\n",
    "test_dataset = get_dataset(test_df, BATCH_SIZE)\n",
    "small_dataset = get_dataset(small_df, BATCH_SIZE)\n",
    "validation_dataset = get_dataset(validation_df, BATCH_SIZE, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9ceedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Examples:\n",
      "    Total: 3113\n",
      "    Positive: 52 (1.67% of total)\n",
      "\n",
      "small Examples:\n",
      "    Total: 311\n",
      "    Positive: 3 (0.96% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [(test_df, \"test\"), (small_df, \"small\")]\n",
    "\n",
    "for df, df_name in dfs:\n",
    "\n",
    "    neg, pos = np.bincount(df['target'])\n",
    "    total = neg + pos\n",
    "    print('{} Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        df_name, total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6920c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import *\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    models = {\"mobilenet\":  (mobilenet_v3.preprocess_input, MobileNetV3Large),\n",
    "              \"xception\":   (xception.preprocess_input, Xception),\n",
    "              \"resnet\": (resnet_v2.preprocess_input, resnet_v2.ResNet50V2) }\n",
    "\n",
    "\n",
    "    def __init__(self,  network_model = \"mobilenet\",\n",
    "                        pooling = \"max\",\n",
    "                        dense_intermediate = -1,\n",
    "                        dropout = 0.5,\n",
    "                        extra_cols_out = 32,\n",
    "                        bias = tf.keras.initializers.Constant(np.log([500 / 30000]))\n",
    "                ):\n",
    "                        \n",
    "        super().__init__()\n",
    "        preprocessor_and_network = self.models[network_model]\n",
    "        self.image_preprocessor = preprocessor_and_network[0]\n",
    "        self.image_feature_extractor = preprocessor_and_network[1](weights='imagenet', include_top = False, pooling = pooling)\n",
    "\n",
    "        self.image_size = IMAGE_SIZE\n",
    "\n",
    "        self.extra_cols_dense = tf.keras.layers.Dense(extra_cols_out, activation = \"relu\", name=\"extra_cols_dense\")\n",
    "        self.extra_cols_flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.use_intermediate = dense_intermediate > 0\n",
    "        if self.use_intermediate:\n",
    "            self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "            self.dense1 = tf.keras.layers.Dense(dense_intermediate, activation=\"relu\", name = \"final_dense_intermediate\")\n",
    "\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "        self.out = tf.keras.layers.Dense(1, activation = \"sigmoid\", bias_initializer=bias, name = \"final_dense\")\n",
    "\n",
    "    def preprocess_images(self, images):\n",
    "        x = self.image_preprocessor(images)\n",
    "        x = tf.image.resize(x, self.image_size)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs):\n",
    "        preprocessed_images = self.preprocess_images(inputs['image'])\n",
    "        image_output = self.image_feature_extractor(preprocessed_images, training = False)\n",
    "\n",
    "        data = inputs['data']\n",
    "        data = self.extra_cols_dense(data)\n",
    "        data = self.extra_cols_flatten(data)\n",
    "\n",
    "        x = tf.concat([data, image_output], axis = -1)\n",
    "        if self.use_intermediate:\n",
    "            x = self.dropout1(x)\n",
    "            x = self.dense1(x)\n",
    "            \n",
    "        x = self.dropout2(x)\n",
    "        out = self.out(x)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "650301dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(network_model = \"mobilenet\", pooling = \"max\", extra_cols_out = 128, dense_intermediate = 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186084c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "weights_model = MyModel(network_model = \"mobilenet\", pooling = \"max\", extra_cols_out = 128, dense_intermediate = 256)\n",
    "weights_model.compile( loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model.load_weights('weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model.evaluate(validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "243e99ab9e43edd82599dfc00766f0e8204baa42021f8d2790e6b99e9ee445ea"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

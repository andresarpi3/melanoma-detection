{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_JPEG.zip\n",
    "!mv ISIC_2020_Training_JPEG.zip data/jpeg.zip\n",
    "!unzip data/jpeg.zip -d data/jpeg\n",
    "!rename.ul jpg jpeg data/jpeg/train/*.jpg\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2020/ISIC_2020_Training_GroundTruth.csv\n",
    "!mv ISIC_2020_Training_GroundTruth.csv data/train.csv\n",
    "!rm data/jpeg.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a82422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 12:19:14.025843: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import MyModel\n",
    "from data_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55880eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'jpeg', 'train')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "TF_PREFIX = 'train'\n",
    "TRAIN_SPLIT = 0.9\n",
    "SMALL_SPLIT = 0.01\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1f0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28013, 7), (3113, 7), (311, 7), (2000, 7))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "original_df = pd.read_csv(TRAIN_CSV).sample(frac=1).reset_index(drop=True)\n",
    "original_df.set_index('image_name', inplace = True)\n",
    "\n",
    "validation_df = original_df.iloc[:2000]\n",
    "original_df = original_df.iloc[2000:]\n",
    "\n",
    "split_point = int(len(original_df) * TRAIN_SPLIT)\n",
    "small_split_point = int(len(original_df) * SMALL_SPLIT)\n",
    "\n",
    "train_df = original_df.iloc[:split_point]\n",
    "test_df = original_df[split_point:]\n",
    "small_df = original_df[:small_split_point]\n",
    "\n",
    "train_df.shape, test_df.shape, small_df.shape, validation_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b85fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CsvTransformer(TRAIN_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6093196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(filename):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "\n",
    "    image_name = tf.strings.split(filename, sep='/')[-1]\n",
    "    image_name = tf.strings.split(image_name, sep='.')[0]\n",
    "    data = transformer.get_data_vector(image_name)\n",
    "    target = tf.cast(transformer.get_vector_from_image_name('target', image_name), dtype=tf.int32)\n",
    "\n",
    "    return {\"image\": image, \n",
    "            \"image_name\": image_name,\n",
    "            \"data\": data}, target\n",
    "\n",
    "def get_dataset(df: pd.DataFrame, images_dir, batch_size: int, cache = True):\n",
    "    filenames = images_dir + '/' + df.index.values + \".jpeg\"\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    ds = ds.map(map_fn)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c959e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = get_dataset(train_df, BATCH_SIZE, cache = True)\n",
    "small_train_dataset = get_dataset(train_df.iloc[:2000], IMAGES_DIR, BATCH_SIZE)\n",
    "test_dataset = get_dataset(test_df, IMAGES_DIR, BATCH_SIZE)\n",
    "small_dataset = get_dataset(small_df, IMAGES_DIR, BATCH_SIZE)\n",
    "validation_dataset = get_dataset(validation_df, IMAGES_DIR, BATCH_SIZE, cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9ceedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Examples:\n",
      "    Total: 3113\n",
      "    Positive: 52 (1.67% of total)\n",
      "\n",
      "small Examples:\n",
      "    Total: 311\n",
      "    Positive: 3 (0.96% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [(test_df, \"test\"), (small_df, \"small\")]\n",
    "\n",
    "for df, df_name in dfs:\n",
    "\n",
    "    neg, pos = np.bincount(df['target'])\n",
    "    total = neg + pos\n",
    "    print('{} Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        df_name, total, pos, 100 * pos / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefd9497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<model.MyModel at 0x7f9429bc5fa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_model = MyModel.create_standard_version(load_weights_path=\"weights/\", compile=True)\n",
    "weights_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 12:22:57.367733: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/98 [=============>................] - ETA: 23s - loss: 0.1197 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 736.2826 - fn: 15.7174 - accuracy: 0.9733 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7237 - prc: 0.0659"
     ]
    }
   ],
   "source": [
    "weights_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stats(dataset, threshold):\n",
    "    predictions, targets = [], []\n",
    "    for el, target in dataset:\n",
    "        preds = weights_model.predict(el)\n",
    "        predictions.append(preds)\n",
    "        targets.append(target.numpy())\n",
    "    \n",
    "    predictions, targets = np.vstack(predictions), np.expand_dims(np.hstack(targets), 1)\n",
    "    print(f\"pred stats - mean: {predictions.mean()}, std: {predictions.std()}\")\n",
    "    deciles = np.percentile(predictions, np.arange(10, 100, 10))\n",
    "    print(f\"pred deciles: {deciles}\")\n",
    "    thresh_predictions = (predictions > threshold).astype(\"int\")\n",
    "    res = tf.math.confusion_matrix(labels=targets.flatten(), predictions=thresh_predictions.flatten())\n",
    "    true_positives, false_positives, true_negatives, false_negatives = res[1, 1], res[0, 1], res[0, 0], res[1, 0]\n",
    "    print(\"true_positives: %d, false_positives: %d, true_negatives: %d, false_negatives: %d\" % (true_positives, false_positives, true_negatives, false_negatives))\n",
    "    sensitivity = true_positives / (true_positives + false_negatives) * 100.0\n",
    "    specificity = true_negatives /(true_negatives + false_positives) * 100.0\n",
    "\n",
    "    print(f\"sensitivity: {sensitivity:.2f}%, specificity: {specificity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(dataset = small_train_dataset, threshold = 0.023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(dataset = test_dataset, threshold = 0.023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(dataset = validation_dataset, threshold = 0.023)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "243e99ab9e43edd82599dfc00766f0e8204baa42021f8d2790e6b99e9ee445ea"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
